{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0bd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dd839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5c1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"E:/bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"E:/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205b3d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 把文字直接输入到tokenizer中，tokenizer处理后的结果作为BERT模型的输入   “pt”表示返回数据格式是pytorch的张量格式\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70fcef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,  7592,  2088,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# token_type_ids表示含义是  是否是一句话，一个字符串中的句子相当于一句话\n",
    "inputs_ = tokenizer(\"Hello, my dog is cute\",\"hello world\", return_tensors=\"pt\")\n",
    "print(inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8ed60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# 超过最大长度的部分，attention_mask变成0\n",
    "inputs__ = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding='max_length', max_length=10)\n",
    "print(inputs__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3746c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "模型的输出有last_hidden_state 模型最后一层输出的隐藏层状态\n",
    "pooler_output  <CLS>对应的最后一层隐藏层的输出\n",
    "hidden_states \n",
    "attentions \n",
    "cross_attentions \n",
    "past_key_values \n",
    "'''\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a39ac77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
      "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
      "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
      "         ...,\n",
      "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
      "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
      "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
      "          2.2184e-02,  5.7299e-01,  6.2331e-02,  5.9586e-02, -9.9965e-01,\n",
      "          5.0146e-02,  4.4756e-01,  9.7612e-01,  3.3989e-02,  8.4494e-01,\n",
      "         -3.6905e-01,  9.8649e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
      "          4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
      "          6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
      "         -3.2187e-01,  8.6670e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
      "         -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
      "         -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1518e-02,\n",
      "         -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3119e-02,\n",
      "          3.6740e-01, -7.3229e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
      "          4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
      "         -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9813e-01,\n",
      "          1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
      "          6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
      "          8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
      "          3.9848e-01,  2.1360e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
      "         -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
      "          3.9138e-01, -2.1884e-02, -9.6331e-02,  3.8545e-01, -3.4136e-01,\n",
      "         -8.0363e-02, -3.2022e-02, -3.6328e-01, -7.8130e-02,  1.9192e-02,\n",
      "         -1.3429e-01, -1.6013e-02, -5.2640e-02, -2.8006e-01,  9.3611e-02,\n",
      "         -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
      "          2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
      "         -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
      "          3.7343e-02, -1.9320e-01,  9.1691e-01,  3.6012e-01,  1.4505e-01,\n",
      "          1.5398e-01, -1.0658e-02, -1.0000e+00, -3.1573e-01, -3.1038e-01,\n",
      "          1.6523e-01, -8.0330e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
      "          9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
      "          2.5317e-01, -2.0185e-01,  2.9533e-02, -2.3162e-01,  3.4632e-01,\n",
      "         -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1135e-02,\n",
      "         -9.6284e-02, -7.6358e-02, -6.5150e-02, -8.9277e-01, -2.3465e-01,\n",
      "          9.1176e-01,  7.0429e-02, -2.1429e-01,  3.8197e-01,  3.5892e-02,\n",
      "         -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
      "          2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
      "          1.8090e-01, -3.2496e-02, -3.0138e-01, -9.7103e-01, -1.3917e-01,\n",
      "          3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8811e-02,  1.3991e-02,\n",
      "         -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
      "          1.5224e-01, -1.8241e-01,  5.5584e-02, -7.7306e-01, -9.9000e-02,\n",
      "          4.7058e-01, -1.7022e-01, -7.7803e-01,  5.2834e-02, -3.7679e-01,\n",
      "         -4.1295e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
      "          5.0384e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
      "          2.8563e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
      "          3.1903e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
      "         -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0618e-03, -4.1152e-01,\n",
      "          3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
      "         -4.8133e-01,  9.3245e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
      "          7.7973e-01,  4.6697e-01, -3.4636e-01,  5.5237e-02,  9.0312e-01,\n",
      "         -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
      "         -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
      "         -2.6726e-01, -5.4679e-01,  9.3923e-04, -1.0502e-02, -4.6837e-01,\n",
      "          3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
      "          8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
      "         -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
      "         -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6622e-02,  8.5571e-01,\n",
      "         -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
      "          8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
      "          3.3430e-01,  2.1397e-01,  3.0629e-02,  9.3878e-01,  9.9986e-01,\n",
      "          9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
      "          9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
      "          2.3398e-02, -1.0000e+00, -6.1938e-02,  1.9563e-01, -9.0552e-01,\n",
      "         -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
      "          8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4073e-01,  9.6085e-01,\n",
      "          1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
      "         -5.9538e-01,  3.7464e-01, -2.1189e-01,  8.8024e-01,  1.9647e-02,\n",
      "         -3.8349e-01, -8.4779e-01,  1.4677e-02, -2.8375e-02, -4.4313e-01,\n",
      "         -9.4966e-01, -6.5704e-02, -7.2326e-02,  6.5967e-01, -1.1504e-01,\n",
      "          2.1876e-01, -5.5254e-01,  9.2218e-02, -5.0583e-01, -5.2826e-02,\n",
      "          5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
      "         -3.1652e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
      "          1.0000e+00,  1.1756e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
      "         -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
      "          2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
      "          7.8268e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
      "         -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
      "         -4.9026e-01, -1.2517e-01, -3.9076e-02,  8.8128e-02, -8.2481e-01,\n",
      "          3.8301e-01,  1.8045e-01,  5.4797e-02,  8.0041e-01, -3.3501e-01,\n",
      "         -3.9115e-01,  1.4233e-01, -9.0141e-02,  3.4585e-01,  4.4044e-01,\n",
      "          3.1045e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
      "         -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0751e-02,  8.3375e-02,\n",
      "         -3.1300e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
      "         -1.4267e-01, -7.4000e-01,  2.2856e-01, -7.9442e-01, -9.8812e-01,\n",
      "          4.3592e-01,  7.7229e-02, -3.8084e-02,  9.9490e-01,  3.2616e-01,\n",
      "          6.7989e-02,  8.2888e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
      "          3.7666e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
      "         -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
      "          2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
      "          7.3959e-02,  3.1274e-02,  1.0000e+00, -7.5631e-01,  3.5396e-01,\n",
      "          5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
      "          3.5377e-02, -4.6061e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
      "          3.4663e-02,  4.5226e-03, -8.8296e-01, -9.8300e-01,  1.6467e-01,\n",
      "          3.3595e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
      "          1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
      "          2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9311e-01,  8.2643e-01,\n",
      "          3.1896e-02,  1.8035e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
      "         -4.1523e-01, -2.0335e-01,  5.0977e-03,  1.0000e+00, -1.3769e-01,\n",
      "          4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
      "          5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1691e-01,  3.3706e-01,\n",
      "         -9.4995e-02,  3.3125e-01, -1.1600e-01,  5.5663e-02,  6.9017e-01,\n",
      "          5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
      "          4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
      "          4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5117e-02, -9.8683e-01,\n",
      "         -3.4659e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
      "          4.0561e-01,  5.1398e-01, -7.1925e-03,  3.7469e-01, -4.9920e-02,\n",
      "         -1.8379e-01,  1.0699e-01,  6.4272e-02,  9.4363e-01, -4.5982e-01,\n",
      "         -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
      "         -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
      "         -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7384e-02,  9.4459e-01,\n",
      "          8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5495e-02,  2.9531e-01,\n",
      "         -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
      "          9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
      "          1.1140e-01, -7.1072e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
      "          7.1901e-03,  9.3813e-02,  1.2718e-01, -4.2176e-01,  6.2383e-01,\n",
      "         -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
      "          5.2774e-01, -6.4999e-02,  3.8218e-02, -1.3764e-01,  1.3114e-01,\n",
      "         -8.2896e-01, -6.2801e-02, -1.3077e-01, -9.9745e-01,  3.8189e-01,\n",
      "         -1.0000e+00, -4.9527e-02, -3.3011e-01, -9.7045e-03,  7.4031e-01,\n",
      "          4.5588e-01, -4.3038e-02, -5.9485e-01,  3.5138e-02,  8.4290e-01,\n",
      "          7.0024e-01,  4.9502e-03,  1.5221e-01, -4.8182e-01,  3.4912e-02,\n",
      "          6.8681e-02,  5.9797e-02,  9.4147e-02,  5.7532e-01,  3.5063e-02,\n",
      "          1.0000e+00, -4.4784e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
      "         -4.8241e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
      "         -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
      "         -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2237e-01,  2.1811e-01,\n",
      "         -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0504e-02,  9.8319e-01,\n",
      "          3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1441e-02,  9.3666e-01,\n",
      "          7.3988e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
      "         -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
      "          2.1140e-01, -3.4107e-02,  8.0942e-01,  4.9842e-03,  8.9624e-01,\n",
      "          6.7184e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
      "         -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
      "          1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
      "         -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
      "          4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
      "         -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0547e-01,\n",
      "          2.5306e-01, -4.0736e-01, -3.4586e-02,  4.0999e-01, -8.3874e-01,\n",
      "         -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
      "         -5.9613e-01, -1.9439e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
      "          5.9324e-01, -1.8357e-02, -6.8001e-03,  2.1554e-01,  8.2484e-01,\n",
      "          8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
      "          2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3627e-03, -3.2062e-02,\n",
      "         -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2644e-01,  6.3986e-01,\n",
      "         -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
      "          6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3169e-01,  5.3484e-02,\n",
      "          8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
      "         -5.3294e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1458e-01,\n",
      "         -6.7958e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
      "         -6.4846e-02, -4.3871e-01,  6.3752e-01, -2.1017e-01, -2.9291e-01,\n",
      "         -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4509e-02,\n",
      "         -2.9022e-01, -2.1883e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
      "         -9.9998e-01,  1.4066e-01,  9.1605e-02,  1.1458e-01, -2.1965e-01,\n",
      "          3.0746e-01, -5.7719e-02, -8.7692e-01, -9.3891e-02,  2.2809e-01,\n",
      "          3.8767e-02, -3.2828e-01, -3.1139e-01,  4.1117e-01,  4.6004e-01,\n",
      "          5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
      "         -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9db9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d66fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Data.Dataset):\n",
    "    \n",
    "    # 初始化MyDataSet对象需要传入参数data和label\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data # ['今天天气很好',1]\n",
    "        self.label = label # [1,0,2]\n",
    "        self.tokenzier = BertTokenizer.from_pretrained(\"E:/bert-base-uncased\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx] # str\n",
    "        label = self.label[idx]\n",
    "        '使用预训练模型的tokenizer分词器，把输入文本数据处理成模型需要的tensor格式'\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding='max_length', max_length=10, trucation=True) # trucation为True表示超过最大长度进行截断\n",
    "        input_ids = inputs.input_ids.squeeze(0) # 默认输出是二维的,转成一维的\n",
    "        token_type_ids = inputs.token_type_ids.squeeze(0)\n",
    "        attention_mask = inputs.attention_mask.squeeze(0)\n",
    "        return input_ids,token_type_ids,attention_mask,label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1eed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = [], []\n",
    "with open(\"E:/testdata.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data_, label_ = line.strip().split('\\t')\n",
    "        data.append(data_)\n",
    "        label.append(int(label_))\n",
    "\n",
    "dataset = MyDataSet(data, label)\n",
    "dataloader = Data.DataLoader(dataset, batch_size=2, shuffle=True)  # torch.utils.data中的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aafe658b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'Module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(MyModule, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Module'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bert后接Linear层\n",
    "\"\"\"\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"E:/bert-base-uncased\")\n",
    "        self.linear = nn.Linear(768, 3) # bert-base-uncased/config.json中hidden_size的值是768  分类结果有3类 \n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask).pooler_output  # .pooler_output获得<CLS>对应的输出向量 维度是[batch,hidden_size]\n",
    "        output = self.linear(output)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = MyModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=le-5) # import torch.optim Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for input_ids, token_type_ids, attention_mask, label in dataloader:\n",
    "        input_ids, token_type_ids, attention_mask, label = input_ids.to(device), token_type_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "        pred = model(input_ids, token_type_ids, attention_mask)\n",
    "        \n",
    "        loss = loss_fn(pred, label)\n",
    "        print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bert后接CNN\n",
    "\"\"\"\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"E:/bert-base-uncased\")\n",
    "        self.conv = nn.Conv2d(1, 3, kernel_size=(2,768))  # 卷积层 输入通道是1，输出通道是3，卷积核大小是2*768\n",
    "        self.linear = nn.Linear(27, 3) # CNN处理得到的输出维度是[batch,27]\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        batch = input_ids.size(0)\n",
    "        '''\n",
    "        CNN的输入应该是4维的，[batch, channel, width, height]\n",
    "        '''\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask).last_hidden_states  # [batch, seq, hidden_size] \n",
    "        output = output.unsqueeze(1) # 在第1维上展开 [batch, 1， seq, hidden_size] \n",
    "        '''\n",
    "        长是hidden_size=768，宽是epoch=10\n",
    "        [（x + 2padding - kernal）/stride ] + 1\n",
    "        长768作为x kernal大小是768带入上面的公式得到 （768+0-768）/1 +1 =0+1 =1\n",
    "        宽10作为x kernal大小是2带入上公式得到（10-2）/1 +1=9\n",
    "        '''\n",
    "        output = self.conv(output) # 输出的维度是[batch, 3, 9, 1]  宽度为9，高度为1   \n",
    "        output = output.view(batch, -1)  # 转化为二维的[batch,***]  得到[batch, 3*9*1]\n",
    "        output = self.linear(output)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bert后接LSTM\n",
    "\"\"\"\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"E:/bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=512, batch_first=True, bidirectional=True)  # 参数信息https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        self.linear = nn.Linear(1024, 3) # LSTM处理得到的输出维度是[batch,1024]\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        batch = input_ids.size(0)\n",
    "        '''\n",
    "        x:[batch, seq]\n",
    "        x=>word2Vex [batch, seq, dim]\n",
    "        '''\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask).last_hidden_states  # [batch, seq, hidden_size] \n",
    "        output,_ = self.lstm(output)  # output维度是[2,10,1024]  10是sequence的长度，1024=512*2\n",
    "        output = output[:, -1, :]  # 处理成[batch,1024]的维度\n",
    "        output = self.linear(output)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用pytorch，最重要的是要弄明白每个层输入和输出的维度\n",
    "\n",
    "nn.LSTM  nn.Linear 需要的维度是什么\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
